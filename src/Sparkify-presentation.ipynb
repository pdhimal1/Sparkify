{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pdsager:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkify</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f22bbf0ceb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"sparkify\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(\"../data/mini_sparkify_event_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data, we will do a vertical show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " artist        | Martha Tilston       \n",
      " auth          | Logged In            \n",
      " firstName     | Colin                \n",
      " gender        | M                    \n",
      " itemInSession | 50                   \n",
      " lastName      | Freeman              \n",
      " length        | 277.89016            \n",
      " level         | paid                 \n",
      " location      | Bakersfield, CA      \n",
      " method        | PUT                  \n",
      " page          | NextSong             \n",
      " registration  | 1538173362000        \n",
      " sessionId     | 29                   \n",
      " song          | Rockpools            \n",
      " status        | 200                  \n",
      " ts            | 1538352117000        \n",
      " userAgent     | Mozilla/5.0 (Wind... \n",
      " userId        | 30                   \n",
      "-RECORD 1-----------------------------\n",
      " artist        | Five Iron Frenzy     \n",
      " auth          | Logged In            \n",
      " firstName     | Micah                \n",
      " gender        | M                    \n",
      " itemInSession | 79                   \n",
      " lastName      | Long                 \n",
      " length        | 236.09424            \n",
      " level         | free                 \n",
      " location      | Boston-Cambridge-... \n",
      " method        | PUT                  \n",
      " page          | NextSong             \n",
      " registration  | 1538331630000        \n",
      " sessionId     | 8                    \n",
      " song          | Canada               \n",
      " status        | 200                  \n",
      " ts            | 1538352180000        \n",
      " userAgent     | \"Mozilla/5.0 (Win... \n",
      " userId        | 9                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(vertical=True, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important column in the data set seems to be the `Page` column. It holds all Sparkify pages that the customers have visited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "| Submit Registration|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|            Register|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"page\").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list above we can use the `Cancellation Confirmation` to indicate if a particular user churned or not. We are building a model that predicts if a user is going to churn given the history of interactions with the service.\n",
    "\n",
    "Therfore, it is important to understand the customers that reached the `Cancellation Confirmation` page. These are the customers that churned, and the task is to build a prediction model that can recognize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we indentified our \"goal\", we can let go of some of the columns that are not needed for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " auth          | Logged In            \n",
      " gender        | M                    \n",
      " itemInSession | 50                   \n",
      " length        | 277.89016            \n",
      " level         | paid                 \n",
      " location      | Bakersfield, CA      \n",
      " method        | PUT                  \n",
      " page          | NextSong             \n",
      " registration  | 1538173362000        \n",
      " sessionId     | 29                   \n",
      " song          | Rockpools            \n",
      " status        | 200                  \n",
      " ts            | 1538352117000        \n",
      " userAgent     | Mozilla/5.0 (Wind... \n",
      " userId        | 30                   \n",
      "-RECORD 1-----------------------------\n",
      " auth          | Logged In            \n",
      " gender        | M                    \n",
      " itemInSession | 79                   \n",
      " length        | 236.09424            \n",
      " level         | free                 \n",
      " location      | Boston-Cambridge-... \n",
      " method        | PUT                  \n",
      " page          | NextSong             \n",
      " registration  | 1538331630000        \n",
      " sessionId     | 8                    \n",
      " song          | Canada               \n",
      " status        | 200                  \n",
      " ts            | 1538352180000        \n",
      " userAgent     | \"Mozilla/5.0 (Win... \n",
      " userId        | 9                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(*['artist','firstName', 'lastName', 'id_copy'])\n",
    "data.show(vertical=True, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are building a model that focuses on a user, remove any null/na values in the userID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8346"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filter((isnan(data['userId'])) | (data['userId'].isNull()) | (data['userId'] == \"\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna(how = 'any', subset = ['userId'])\n",
    "data = data.filter(data[\"userId\"] != \"\") \n",
    "data = data.filter(data['userId'].isNotNull())\n",
    "data.filter((isnan(data['userId'])) | (data['userId'].isNull()) | (data['userId'] == \"\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The times in `registration` and `ts` column are given in milliseconds, we will convert those to seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "| registration|           ts|\n",
      "+-------------+-------------+\n",
      "|1.538173362E9|1.538352117E9|\n",
      "| 1.53833163E9| 1.53835218E9|\n",
      "|1.538173362E9|1.538352394E9|\n",
      "| 1.53833163E9|1.538352416E9|\n",
      "|1.538173362E9|1.538352676E9|\n",
      "| 1.53833163E9|1.538352678E9|\n",
      "| 1.53833163E9|1.538352886E9|\n",
      "|1.538173362E9|1.538352899E9|\n",
      "|1.538173362E9|1.538352905E9|\n",
      "|1.538173362E9|1.538353084E9|\n",
      "| 1.53833163E9|1.538353146E9|\n",
      "| 1.53833163E9| 1.53835315E9|\n",
      "|1.538173362E9|1.538353218E9|\n",
      "| 1.53833163E9|1.538353375E9|\n",
      "| 1.53833163E9|1.538353376E9|\n",
      "|1.538173362E9|1.538353441E9|\n",
      "| 1.53833163E9|1.538353576E9|\n",
      "|1.537365219E9|1.538353668E9|\n",
      "|1.538173362E9|1.538353687E9|\n",
      "| 1.53833163E9|1.538353744E9|\n",
      "+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_unit_udf = F.udf(lambda x: float(x/1000), DoubleType())\n",
    "data = data.withColumn(\"registration\", time_unit_udf(\"registration\")). \\\n",
    "    withColumn(\"ts\", time_unit_udf(\"ts\"))\n",
    "\n",
    "data.select('registration', 'ts').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above we are deriving the label `churn` as a function of the `Page` column. There are only two values to churn, churned or not churned. Therefore this is going to be a boolean column represented by 1 for churned and 0 for not churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|              churn|\n",
      "+-------+-------------------+\n",
      "|  count|                 11|\n",
      "|   mean|0.09090909090909091|\n",
      "| stddev|0.30151134457776363|\n",
      "|    min|                  0|\n",
      "|    max|                  1|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancelation_event_udf = F.udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "data = data.withColumn(\"churn\", cancelation_event_udf(\"page\"))\n",
    "data.filter(data['userId'] == 125).describe('churn').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that a particular user will have multiple entries, since this is a database of user activities log. A user may have visitied number of other pages before reaching the \"Cancellation\" page. For a user that has churned, we want to make sure that all of his/her logs record 1 for the churn column. For this we will use the Window function from `sql.window`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as Fsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"userId\") \\\n",
    "        .rangeBetween(Window.unboundedPreceding,\n",
    "                      Window.unboundedFollowing)\n",
    "data = data.withColumn(\"churn\", Fsum(\"churn\").over(window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|summary|churn|\n",
      "+-------+-----+\n",
      "|  count|   11|\n",
      "|   mean|  1.0|\n",
      "| stddev|  0.0|\n",
      "|    min|    1|\n",
      "|    max|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter(data['userId'] == 125).describe('churn').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many unique users we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"userID\").dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total churn instances, again a user has multiple entries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44864"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filter(data['churn'] == 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a label dataframe that has one entry per each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|userId|label|\n",
      "+------+-----+\n",
      "|100010|    0|\n",
      "|200002|    0|\n",
      "|   125|    1|\n",
      "|   124|    0|\n",
      "|    51|    1|\n",
      "|     7|    0|\n",
      "|    15|    0|\n",
      "|    54|    1|\n",
      "|   155|    0|\n",
      "|100014|    1|\n",
      "|   132|    0|\n",
      "|   154|    0|\n",
      "|   101|    1|\n",
      "|    11|    0|\n",
      "|   138|    0|\n",
      "|300017|    0|\n",
      "|100021|    1|\n",
      "|    29|    1|\n",
      "|    69|    0|\n",
      "|   112|    0|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = data \\\n",
    "    .select('userId', col('churn').alias('label')) \\\n",
    "    .dropDuplicates()\n",
    "label.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users that have been using the service tend to stay with the service, even as a paying customer, than those that recently signed up.\n",
    "\n",
    "Every company selling products and services to customers have an idea of a \"lifetime value\" of a customer. With that in mind we create our first feature.\n",
    "\n",
    "We converted the times to seconds above. After gaining the lifetime of a user, we convert that to days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as Fsum, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_since_registration = data \\\n",
    "    .select('userId', 'registration', 'ts') \\\n",
    "    .withColumn('lifetime', (data.ts - data.registration)) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg({'lifetime': 'max'}) \\\n",
    "    .withColumnRenamed('max(lifetime)', 'lifetime') \\\n",
    "    .select('userId', (col('lifetime') / 3600 / 24).alias('lifetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|           lifetime|\n",
      "+-------+-------------------+\n",
      "|  count|                225|\n",
      "|   mean|   79.8456834876543|\n",
      "| stddev|  37.66147001861254|\n",
      "|    min|0.31372685185185184|\n",
      "|    max|  256.3776736111111|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_since_registration.describe('lifetime').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bussiness big and small not only rely on their customers coming back for more good and services, but also referring the bussiness to their friends and family. It is in our nature to refer things that we like. With this idea in mind we will create another feature.\n",
    "\n",
    "Again we will user the \"Page\" column specifically looking for the `Add Friend` page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "referring_friends = data \\\n",
    "    .select('userID', 'page') \\\n",
    "    .where(data.page == 'Add Friend') \\\n",
    "    .groupBy('userID') \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count', 'add_friend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        add_friend|\n",
      "+-------+------------------+\n",
      "|  count|               206|\n",
      "|   mean|20.762135922330096|\n",
      "| stddev|20.646779074405007|\n",
      "|    min|                 1|\n",
      "|    max|               143|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "referring_friends.describe('add_friend').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
